
Deployment 

# Awareness HA Deployment

## NFS: RHEL/CentOS/ Debian/Ubuntu 
==============================================================================


1.  **NFS** allows local access to remote files.
2.  It uses standard **client**/**server** architecture for file sharing
    between all \***nix** based machines.
3.  With **NFS** it is not necessary that both machines run on the same
    **OS**.

![](https://awareness-docs.s3.us-east-2.amazonaws.com/_static/images/nfs.png)
4.  With the help of **NFS** we can configure **centralized storage**
    solutions.
5.  Users get their **data** irrespective of physical location.
6.  No manual **refresh** needed for new files.
7.  Newer version of **NFS** also supports **acl**, **pseudo** root
    mounts.
8.  Can be secured with **Firewalls** and **Kerberos**.

##### NFS Services

Its a **System V-launched** service. The **NFS** server package includes
three facilities, included in the **portmap** and **nfs-utils**
packages.

1.  **portmap** : It maps calls made from other machines to the correct
    **RPC** service (not required with **NFSv4**).
2.  **nfs**: It translates remote **file sharing** requests into
    requests on the local file system.
3.  **rpc.mountd**: This service is responsible for **mounting** and
    **unmounting** of file systems.

##### Important Files for NFS Configuration

1.  **/etc/exports** : Its a main configuration file of **NFS**, all
    exported **files** and **directories** are defined in this file at
    the **NFS Server** end.
2.  **/etc/fstab** : To mount a **NFS directory** on your system across
    the **reboots**, we need to make an entry in **/etc/fstab**.
3.  **/etc/sysconfig/nfs** : Configuration file of **NFS** to control on
    which port **rpc** and other services are **listening**.

#### Setup and Configure NFS Mounts on Linux Server

To setup **NFS** mounts, we’ll be needing at least two
**Linux**/**Unix** machines. Here in this tutorial, I’ll be using two
servers.

1.  **NFS Server**: nfsserver.example.com with IP-**192.168.0.100**
2.  **NFS Client** : nfsclient.example.com with IP-**192.168.0.101**

##### Installing NFS Server and NFS Client

We need to install **NFS** packages on our **NFS Server** as well as on
**NFS Client** machine. We can install it via “**yum**” (**Red Hat**
Linux) and “**apt-get**” (**Debian** and **Ubuntu**) package installers.

    [root@nfsserver ~]# yum install nfs-utils nfs-utils-lib
    [root@nfsserver ~]# yum install portmap (not required with NFSv4)

    [root@nfsserver ~]# apt-get install nfs-utils nfs-utils-lib

Now start the **services** on both machines.

    [root@nfsserver ~]# /etc/init.d/portmap start
    [root@nfsserver ~]# /etc/init.d/nfs start
    [root@nfsserver ~]# chkconfig --level 35 portmap on
    [root@nfsserver ~]# chkconfig --level 35 nfs on

After installing packages and starting services on both the machines, we
need to configure both the machines for file sharing.

#### Setting Up the NFS Server

First we will be configuring the **NFS** server.

##### Configure Export directory

For sharing a directory with **NFS**, we need to make an entry in
“**/etc/exports**” configuration file. Here I’ll be creating a new
directory named “**nfsshare**” in “**/**” partition to share with
**client server**, you can also share an already existing directory with
NFS.

    [root@nfsserver ~]# mkdir /nfsshare

Now we need to make an entry in “**/etc/exports**” and **restart** the
services to make our directory shareable in the network.

    [root@nfsserver ~]# vi /etc/exports

    /nfsshare 192.168.0.101(rw,sync,no_root_squash)

In the above example, there is a directory in **/** partition named
“**nfsshare**” is being shared with client IP “**192.168.0.101**” with
**read** and **write** (**rw**) privilege, you can also use **hostname**
of the client in the place of **IP** in above example.

##### NFS Options

Some other options we can use in “**/etc/exports**” file for file
sharing is as follows.

1.  **ro**: With the help of this option we can provide **read only
    access** to the shared files i.e **client** will only be able to
    **read**.
2.  **rw**: This option allows the **client server** to both **read**
    and **write** access within the shared directory.
3.  **sync**: Sync confirms requests to the shared directory only once
    the **changes** have been committed.
4.  **no\_subtree\_check**: This option prevents the **subtree**
    checking. When a shared directory is the subdirectory of a larger
    file system, **nfs** performs scans of every directory above it, in
    order to verify its permissions and details. Disabling the
    **subtree** check may increase the reliability of **NFS**, but
    reduce **security**.
5.  **no\_root\_squash**: This phrase allows **root** to **connect** to
    the designated directory.

For more options with “**/etc/exports**“, you are recommended to read
the **man pages** for **export**.

#### Setting Up the NFS Client

After configuring the **NFS** server, we need to **mount** that shared
directory or partition in the **client** server.

##### Mount Shared Directories on NFS Client

Now at the **NFS client** end, we need to **mount** that directory in
our server to access it locally. To do so, first we need to find out
that shares available on the remote server or NFS Server.

    [root@nfsclient ~]# showmount -e 192.168.0.100

    Export list for 192.168.0.100:
    /nfsshare 192.168.0.101

Above command shows that a directory named “**nfsshare**” is available
at “**192.168.0.100**” to share with your server.

##### Mount Shared NFS Directory

To **mount** that shared **NFS** directory we can use following mount
command.

    [root@nfsclient ~]# mount -t nfs 192.168.0.100:/nfsshare /mnt/nfsshare

The above command will mount that shared directory in
“**/mnt/nfsshare**” on the client server. You can verify it following
command.

    [root@nfsclient ~]# mount | grep nfs

    sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw)
    nfsd on /proc/fs/nfsd type nfsd (rw)
    192.168.0.100:/nfsshare on /mnt type nfs (rw,addr=192.168.0.100)

The above mount command mounted the **nfs shared directory** on to **nfs
client** temporarily, to mount an NFS directory **permanently** on your
system across the **reboots**, we need to make an entry in
“**/etc/fstab**“.

    [root@nfsclient ~]# vi /etc/fstab

Add the following new line as shown below.

    192.168.0.100:/nfsshare /mnt  nfs defaults 0 0

#### Test the Working of NFS Setup

We can test our **NFS server setup** by creating a **test file** on the
server end and check its availability at **nfs client** side or
vice-versa.

##### At the nfsserver end

I have created a new text file named “**nfstest.txt**’ in that shared
directory.

    [root@nfsserver ~]# cat > /nfsshare/nfstest.txt

    This is a test file to test the working of NFS server setup.

##### At the nfsclient end

Go to that shared directory in **client server** and you’ll find that
shared file without any manual refresh or service restart.

    [root@nfsclient]# ll /mnt/nfsshare
    total 4
    -rw-r--r-- 1 root root 61 Sep 21 21:44 nfstest.txt
    root@nfsclient ~]# cat /mnt/nfsshare/nfstest.txt
    This is a test file to test the working of NFS server setup.

#### Removing the NFS Mount

If you want to **unmount** that shared directory from your server after
you are done with the file sharing, you can simply **unmount** that
particular directory with “**umount**” command. See this example below.

    root@nfsclient ~]# umount /mnt/nfsshare

You can see that the mounts were removed by then looking at the
filesystem again.

    [root@nfsclient ~]# df -h -F nfs

You’ll see that those shared directories are not available any more.

##### Important commands for NFS

Some more important commands for **NFS**.

1.  **showmount -e** : Shows the available **shares** on your local
    machine
2.  **showmount -e** **\<server-ip or hostname\>**: Lists the available
    **shares** at the **remote** server
3.  **showmount -d** : Lists all the **sub directories**
4.  **exportfs -v** : Displays a list of shares **files** and
    **options** on a server
5.  **exportfs -a** : Exports all shares listed in **/etc/exports**, or
    given name
6.  **exportfs -u** : Unexports all shares listed in **/etc/exports**,
    or given name
7.  **exportfs -r** : Refresh the server’s list after modifying
    **/etc/exports**




## Create a swarm Cluster 
===========================

After you complete the (https://docs.docker.com/engine/swarm/swarm-tutorial/) steps,
you’re ready to create a swarm. Make sure the Docker Engine daemon is
started on the host machines.

### Init Swarm Manager Node  
=============================



1.  Open a terminal and ssh into the machine where you want to run your
    manager node. This tutorial uses a machine named
    `manager1`. If you use Docker Machine, you can
    connect to it via SSH using the following command:

    ```
    $ docker-machine ssh manager1
    ```

2.  Run the following command to create a new swarm:

    ```
    $ docker swarm init --advertise-addr <MANAGER-IP>
    ```

    > **Note**: If you are using Docker Desktop for Mac or Docker
    > Desktop for Windows to test single-node swarm, simply run
    > `docker swarm init` with no arguments. There
    > is no need to specify `--advertise-addr` in
    > this case. To learn more, see the topic on how to [Use Docker
    > Desktop or Mac or Docker Desktop for
    > Windows](https://docs.docker.com/engine/swarm/swarm-tutorial/#use-docker-for-mac-or-docker-for-windows)
    > with Swarm.

    In the tutorial, the following command creates a swarm on the
    `manager1` machine:

    ```
    $ docker swarm init --advertise-addr 192.168.99.100
    ```
    Swarm initialized: current node (dxn1zf6l61qsb1josjja83ngz) is now a manager.

    To add a worker to this swarm, run the following command:
     
        ```
        $ docker swarm join \
        --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
        192.168.99.100:2377
        ```
    To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
   

    The `--advertise-addr` flag configures the
    manager node to publish its address as
    `192.168.99.100`. The other nodes in the swarm
    must be able to access the manager at the IP address.

    The output includes the commands to join new nodes to the swarm.
    Nodes will join as managers or workers depending on the value for
    the `--token` flag.

3.  Run `docker info` to view the current state of
    the swarm:

    ```
    $ docker info

    Containers: 2
    Running: 0
    Paused: 0
    Stopped: 2
      ...snip...
    Swarm: active
      NodeID: dxn1zf6l61qsb1josjja83ngz
      Is Manager: true
      Managers: 1
      Nodes: 1
      ...snip...
    ```

4.  Run the `docker node ls` command to view
    information about nodes:

    ```
    $ docker node ls

    ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
    dxn1zf6l61qsb1josjja83ngz *  manager1  Ready   Active        Leader
    ```

    The `*` next to the node ID indicates that
    you’re currently connected on this node.

    Docker Engine swarm mode automatically names the node for the
    machine host name.


### Add nodes to the swarm 
=============================


1.  Open a terminal and ssh into the machine where you want to run a
    worker node. This tutorial uses the name
    `worker1`.

2.  Run the command produced by the
    `docker swarm init` output from the [Create a
    swarm](https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/)
    tutorial step to create a worker node joined to the existing swarm:

    ```
    $ docker swarm join \
      --token  SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
      192.168.99.100:2377
    ```

    This node joined a swarm as a worker.


    If you don’t have the command available, you can run the following
    command on a manager node to retrieve the join command for a worker:

    ```
    $ docker swarm join-token worker
    ```
    To add a worker to this swarm, run the following command:
    ```
        docker swarm join \
        --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
        192.168.99.100:2377
    ```

3.  Open a terminal and ssh into the machine where you want to run a
    second worker node. This tutorial uses the name
    `worker2`.

4.  Run the command produced by the
    `docker swarm init` output from the [Create a
    swarm](https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/)
    tutorial step to create a second worker node joined to the existing
    swarm:

    ```
    $ docker swarm join \
      --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
      192.168.99.100:2377
    ```
   
 This node joined a swarm as a worker.
   
5.  Open a terminal and ssh into the machine where the manager node runs
    and run the `docker node ls` command to see the
    worker nodes:

     ```
     $ docker node ls

    ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
    03g1y59jwfg7cf99w4lt0f662    worker2   Ready   Active
    9j68exjopxe7wfl6yuxml7a7j    worker1   Ready   Active
    dxn1zf6l61qsb1josjja83ngz *  manager1  Ready   Active        Leader
    ```

    The `MANAGER` column identifies the manager
    nodes in the swarm. The empty status in this column for
    `worker1` and `worker2`
    identifies them as worker nodes.

    Swarm management commands like `docker node ls`
    only work on manager nodes.


## Clone Zisoft awareness  project

   Run the command where /zisoft-ha/ is shared-FS:

    $ cd /zisoft-ha/
    $ git clone https://gitlab.com/zisoft/awareness.git
    $ cd awareness
    $ zisoft build --docker --sass --app --ui --composer
    $ zisoft package
    $ zisoft deploy --prod
    
## Deploy Zisoft awareness offline project

   Run the command :

    $ cd /zisoft-ha/
    $ zisoft deploy --prod
 
# Awareness Offline Deployment


## Prerequisites

1.  Docker swarm mode  active :
    
  Run the command :


    $ docker info

Check if swarm mode is active


![](https://awareness-docs.s3.us-east-2.amazonaws.com/_static/images/swarm-active.png)

2.  Nodejs, Npm ,Unzip Package installed.

3.  Offline zisoft-xxxxxxx.Zip file downloaded from release artifacts.


## Extract zisoft-xxxxxxx.Zip file 

   Run the command :

    $ unzip zisoft-xxxxxxx.zip

## Load Zisoft awareness docker images

  Run the command :

    $ cd zisoft-xxxxxxx
    $ zisoft load 

## Deploy Zisoft awareness offline project

   Run the command :

    $ zisoft deploy --offline
 


 

